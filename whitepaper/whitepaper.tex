%	This is written by Zhiyang Ong as a template for writing reports.

%	The MIT License (MIT)

%	Copyright (c) <2014> <Zhiyang Ong>

%	Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

%	The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

%	THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

%	Email address: echo "cukj -wb- 23wU4X5M589 TROJANS cqkH wiuz2y 0f Mw Stanford" | awk '{ sub("23wU4X5M589","F.d_c_b. ") sub("Stanford","d0mA1n"); print $5, $2, $8; for (i=1; i<=1; i++) print "6\b"; print $9, $7, $6 }' | sed y/kqcbuHwM62z/gnotrzadqmC/ | tr 'q' ' ' | tr -d [:cntrl:] | tr -d 'ir' | tr y "\n"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	Preamble.
\documentclass[letter,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%	Importing LaTeX source files, without quoting the ".tex" extension.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	File containing the LaTeX preamble.
\input{./others/preamble_section}





% definition of new \LaTeX command for the citation: \cite{Cimatti08} and \cite{Barrett09}
% This allows mathematical/logic symbols to be typeset with the font ``Zapf Chancery'' in ``\LaTeX\ math mode''. To typeset symbols in such font, try: \mathpzc{ABCdef123}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start of document
\begin{document}
\title{White Paper on Obstacle Detection via a Stereoscopic Camera System, Using Deep Learning}
\date{\today}
\author{Zhiyang Ong\thanks{Email correspondence to: \href{mailto:ongz@acm.org}{ongz@acm.org}}, Michael Bass\thanks{Email correspondence to: \href{mailto:baklava.akbar@falafel.palestine}{baklava.akbar@falafel.palestine}}, Khaled Nakhleh\thanks{Email correspondence to: \href{mailto:khaled.jkn@gmail.com}{khaled.jkn@gmail.com}}, \\
Drupad Khublani\thanks{Email correspondence to: \href{mailto:dkhublani@tamu.edu}{dkhublani@tamu.edu}}, and Venkata Pydimarri\thanks{Email correspondence to: \href{mailto:lassi@barfi.in}{lassi@barfi.in}} \\
Department of Electrical and Computer Engineering \\
College of Engineering \\
Texas A\&M University
}
%College of Engineering \\
%Texas A\&M University
%\author{Zhiyang Ong\thanks{Email correspondence to: \href{mailto:email-id@domain.url}{email-id@domain.url}} and Michael Bass and Khaled Nakhleh and Drupad Khublani and Venkata Pydimarri\\
%Department of Electrical and Computer Engineering \\
%College of Engineering \\
%Texas A&M University
%}
\maketitle


%\begin{abstract} 
%Insert abstract here. More stuff to be included.
%\end{abstract}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Section 1}
%\label{sec:mysection1}
%
%There are a significant amount of references for helping people to learn \LaTeX \cite{Voss2011,vanDongen2012,Syropoulos2003,Raymond2004,Mittelbach2004,Lamport1994,Krishnan2003,Krantz2001,Kottwitz2011,Koranne2011,Kopka2004,Knuth1999,Hoenig1998,Higham1998,Haralambous2007,Griffiths1997,Gratzer2007,Goossens2007,Goossens1999,Goossens1997,Diller1999,Bindner2011,Berry2009,UITCambridge2011,Scharrer2011,Pakin2008,Cormen2010,Syropoulos2004,Hamalainen2006} and related information/technologies.
%



%We are enhancing a rover (see Figure \ref{fig:Isometricprojectionofthevideocameraenhancedrover}), or robotic cyber-physical system, to have stereoscopic vision, so that we can carry out data acquisition and binary classification of whether the rover has detected static or dynamic obstacles. To modify the rover, we mounted video cameras and a laptop to sample images from the video cameras, and perform real-time data acquisition by sampling greyscale images from the video cameras. Subsequently, our {\it Python} program will implement binary classifiers for the the aforementioned tasks. The classifiers will use deep learning \cite{Buduma2017,Ketkar2017,Pattanayak2017,Goodfellow2016,Wang2016,Bengio2015,Cho2014,Goodfellow2014}, specifically an inception-based convolution neural network (CNN). We plan to enhance the classifer by using one-shot obstacle recognition/detection.
We are enhancing a rover (see Figure \ref{fig:Isometricprojectionofthevideocameraenhancedrover}) to have stereoscopic vision, so that we can carry out data acquisition and binary classification of whether the rover has detected static or dynamic obstacles. To modify the rover, we mounted video cameras and a laptop to sample images from the video cameras, and perform real-time data acquisition by sampling greyscale images from the video cameras. Subsequently, our {\it Python} program will implement binary classifiers for the the aforementioned tasks. The classifiers will use deep learning \cite{Buduma2017,Ketkar2017,Pattanayak2017,Goodfellow2016,Wang2016,Bengio2015,Cho2014,Goodfellow2014}, specifically an inception-based convolution neural network (CNN). We plan to enhance the classifer by using one-shot obstacle recognition/detection. {\Huge \bf Fix this description!!!}


\begin{figure}[h]
\centering 
\includegraphics[height=1.5in]{./pics/my_figure}
\caption{Isometric projection of the video camera-enhanced rover}
\label{fig:Isometricprojectionofthevideocameraenhancedrover}
\end{figure}


We aim to perform obstacle detection for a rover traveling at a speed of 20 m/s. We aim to classify static obstacles 30 m away, so that the controller of the rover has time to manipulate the rover away from the obstacles. In addition, we aim to classify moving obstacles at speeds greater than 0.2 m/s. {\Huge \bf Fix this description!!!}


%https://arxiv.org/abs/1512.00567v3


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Work-Breakdown Structure and Project Status}
\label{sec:WorkBreakdownStructureAndProjectStatus}

The initial distribution of tasks and project status are described as follows: \vspace{-0.3cm}
\begin{enumerate} \itemsep -4pt
\item Michael Bass modified a rover to mount two video cameras and a laptop, so that 
\item Khaled Nakhleh develop a {\it Python script} to sample greyscale images from video cameras mounted on the robotic cyber-physical system.
\item Drupad Khublani and Venkata Pydimarri suggested some techniques, and will examine CNN-based techniques for detection of static and dynamic obstacles.
\item Zhiyang Ong wrote the white paper.
\end{enumerate}

Moving forward, Khaled Nakhleh and Michael Bass will develop {\it Python} scripts for preprocessing (e.g., decrease the resolution of the images) of the greyscale images from the video cameras, for statistical analysis of the experimental data, and for visualizing experimental data so that we can obtain figures/plots to include in the final report. Drupad Khublani and Venkata Pydimarri will implement the {\it Python} scripts for deep learning, via a CNN, that will classify the sampled greyscale images into obstacle detected or no obstacle detected. Lastly, Zhiyang Ong would be in charge of writing the final report, with assistance from other team members.










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Alternate Plan}
%\label{sec:AlternatePlan}
%
%Data sets to use in lieu of poor, failed, and/or untimely data acquisition, and preprocessing of data set: \vspace{-0.3cm}
%\begin{enumerate} \itemsep -4pt
%\item Manuela Chessa's GENUA PESTO -- GENoa hUman Active fixation database: PEripersonal space STereoscopic images and grOund truth disparity: \url{https://www.manuelachessa.it/sample-page/benchmarking-datasets-2/}
%\item The KITTI Vision Benchmark Suite: \url{http://www.cvlibs.net/datasets/kitti/eval_object.php}
%\item Mapillary Vistas Dataset: \url{https://www.mapillary.com/dataset/vistas?pKey=cc5dEAyQECBFF9MN3MbdZA&lat=20&lng=0&z=1.5}
%\item ApolloScape: \url{http://apolloscape.auto/}
%\item BDD100K self-driving dataset:url{https://www.therobotreport.com/uc-berkeley-opens-self-driving-dataset-bdd100k/}: \vspace{-0.3cm}
%	\begin{enumerate} \itemsep -2pt
%	\item \url{http://bdd-data.berkeley.edu/} and \url{https://deepdrive.berkeley.edu/}
%	\end{enumerate}
%\item nuScenes: \url{https://www.nuscenes.org/}
%\item Cityscapes Dataset: \url{https://www.cityscapes-dataset.com/}
%\item Top Open-Source Lidar Driving Data Sets: \url{https://mighty.ai/blog/top_open_source_lidar_driving_datasets/}
%\item Apollo Data Open Platform : \url{http://data.apollo.auto/?locale=en-us&lang=en}
%\end{enumerate}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%	End of document
%
%	Inserting references
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	Beginning of BACK MATTER: bibliography, indexes and colophon
%\backmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{\linespread{1}
\bibliographystyle{plain}
\bibliography{./references/references}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}